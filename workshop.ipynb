{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "from os import listdir\n",
    "import nltk\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, TimeDistributed, Dense\n",
    "from tensorflow.keras.layers import Embedding, GRU\n",
    "import gensim\n",
    "import gensim.downloader as model_api\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-transparency",
   "metadata": {},
   "source": [
    "# 1. Sentiment analysis\n",
    "\n",
    "Using the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), we want to do a regression model that predict the ratings are on a 1-10 scale. You have an example train and test set in the `dataset` folder.\n",
    "\n",
    "### 1.1 Regression Model\n",
    "\n",
    "Use a feedforward neural network and NLP techniques we've seen up to now to train the best model you can on this dataset\n",
    "\n",
    "### 1.2 RNN model\n",
    "\n",
    "Train a RNN to do the sentiment analysis regression. The RNN should consist simply of an embedding layer (to make word IDs into word vectors) a recurrent blocks (GRU or LSTM) feeding into an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Regrssion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_files = listdir(\"dataset/aclImdb/train/pos\")\n",
    "neg_train_files = listdir(\"dataset/aclImdb/train/neg\")\n",
    "\n",
    "pos_test_files = listdir(\"dataset/aclImdb/test/pos\")\n",
    "neg_test_files = listdir(\"dataset/aclImdb/test/neg\")\n",
    "\n",
    "def get_reviews(target, rev, files): \n",
    "    x = []\n",
    "    x_line = []\n",
    "\n",
    "    for file in files:\n",
    "        with open (f\"dataset/aclImdb/{target}/{rev}/{file}\", encoding=\"utf8\") as opened_file:\n",
    "            rating = file.split(\"_\")[1].split(\".\")[0]\n",
    "\n",
    "            for line in opened_file:\n",
    "                x_line = []\n",
    "                x_line.append(line)\n",
    "                x_line.append(rating)\n",
    "                x.append(x_line)\n",
    "                \n",
    "    return x\n",
    "\n",
    "train_pos = pd.DataFrame(columns=[\"review\", \"rating\"], data=get_reviews(\"train\", \"pos\", pos_train_files))\n",
    "train_neg = pd.DataFrame(columns=[\"review\", \"rating\"], data=get_reviews(\"train\", \"neg\", neg_train_files))\n",
    "\n",
    "test_pos = pd.DataFrame(columns=[\"review\", \"rating\"], data=get_reviews(\"test\", \"pos\", pos_test_files))\n",
    "test_neg = pd.DataFrame(columns=[\"review\", \"rating\"], data=get_reviews(\"test\", \"neg\", neg_test_files))\n",
    "\n",
    "train_df = pd.concat([train_pos, train_neg], ignore_index=True)\n",
    "test_df = pd.concat([test_pos, test_neg], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              review rating\n",
       "0  For a movie that gets no respect there sure ar...      9\n",
       "1  Bizarre horror movie filled with famous faces ...      8\n",
       "2  A solid, if unremarkable film. Matthau, as Ein...      7\n",
       "3  It's a strange feeling to sit alone in a theat...      8\n",
       "4  You probably all already know this by now, but...     10"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>For a movie that gets no respect there sure ar...</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Bizarre horror movie filled with famous faces ...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>It's a strange feeling to sit alone in a theat...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>You probably all already know this by now, but...</td>\n      <td>10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Lemmatizer from NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function that receive a list of words and do lemmatization:\n",
    "def lemma_stem_text(words_list):\n",
    "    # Lemmatizer\n",
    "    text = [lemmatizer.lemmatize(token.lower()) for token in words_list]\n",
    "    text = [lemmatizer.lemmatize(token.lower(), \"v\") for token in text]\n",
    "    return text\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#Creating a function for cleaning of data\n",
    "def clean_text(raw_text):\n",
    "    # 1. remove HTML tags\n",
    "    raw_text = BeautifulSoup(raw_text).get_text() \n",
    "    \n",
    "    # 2. removing all non letters from text\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text) \n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                           \n",
    "    \n",
    "    # 4. Create variable which contain set of stopwords\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    stops_indo = set(stopwords.words(\"indonesian\"))\n",
    "    stops.update(stops_indo)\n",
    "    \n",
    "    # 5. Remove stop word & returning   \n",
    "    words_tmp = [w for w in words if not w in stops]\n",
    "\n",
    "    # 6. Apply lemmatization function\n",
    "    words_lemm = lemma_stem_text(words_tmp)\n",
    "\n",
    "    # 7. Finalize\n",
    "    return [w for w in words_lemm]\n",
    "\n",
    "\n",
    "clean_words = []\n",
    "for i in range(len(train_df['review'])):\n",
    "    res = clean_text(train_df['review'][i])\n",
    "    res_len = len(res)\n",
    "    clean_words.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                  review rating  \\\n",
       "0      For a movie that gets no respect there sure ar...      9   \n",
       "1      Bizarre horror movie filled with famous faces ...      8   \n",
       "2      A solid, if unremarkable film. Matthau, as Ein...      7   \n",
       "3      It's a strange feeling to sit alone in a theat...      8   \n",
       "4      You probably all already know this by now, but...     10   \n",
       "...                                                  ...    ...   \n",
       "24995  My comments may be a bit of a spoiler, for wha...      3   \n",
       "24996  The \"saucy\" misadventures of four au pairs who...      4   \n",
       "24997  Oh, those Italians! Assuming that movies about...      1   \n",
       "24998  Eight academy nominations? It's beyond belief....      3   \n",
       "24999  Not that I dislike childrens movies, but this ...      3   \n",
       "\n",
       "                                             clean_words  \n",
       "0      [movie, get, respect, sure, lot, memorable, qu...  \n",
       "1      [bizarre, horror, movie, fill, famous, face, s...  \n",
       "2      [solid, unremarkable, film, matthau, einstein,...  \n",
       "3      [strange, feel, sit, alone, theater, occupy, p...  \n",
       "4      [probably, already, know, additional, episode,...  \n",
       "...                                                  ...  \n",
       "24995  [comment, may, bite, spoiler, worth, stop, car...  \n",
       "24996  [saucy, misadventure, four, au, pair, arrive, ...  \n",
       "24997  [oh, italian, assume, movie, aristocrat, weird...  \n",
       "24998  [eight, academy, nomination, beyond, belief, t...  \n",
       "24999  [dislike, childrens, movie, tearjerker, redeem...  \n",
       "\n",
       "[25000 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>rating</th>\n      <th>clean_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>For a movie that gets no respect there sure ar...</td>\n      <td>9</td>\n      <td>[movie, get, respect, sure, lot, memorable, qu...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Bizarre horror movie filled with famous faces ...</td>\n      <td>8</td>\n      <td>[bizarre, horror, movie, fill, famous, face, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n      <td>7</td>\n      <td>[solid, unremarkable, film, matthau, einstein,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>It's a strange feeling to sit alone in a theat...</td>\n      <td>8</td>\n      <td>[strange, feel, sit, alone, theater, occupy, p...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>You probably all already know this by now, but...</td>\n      <td>10</td>\n      <td>[probably, already, know, additional, episode,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24995</th>\n      <td>My comments may be a bit of a spoiler, for wha...</td>\n      <td>3</td>\n      <td>[comment, may, bite, spoiler, worth, stop, car...</td>\n    </tr>\n    <tr>\n      <th>24996</th>\n      <td>The \"saucy\" misadventures of four au pairs who...</td>\n      <td>4</td>\n      <td>[saucy, misadventure, four, au, pair, arrive, ...</td>\n    </tr>\n    <tr>\n      <th>24997</th>\n      <td>Oh, those Italians! Assuming that movies about...</td>\n      <td>1</td>\n      <td>[oh, italian, assume, movie, aristocrat, weird...</td>\n    </tr>\n    <tr>\n      <th>24998</th>\n      <td>Eight academy nominations? It's beyond belief....</td>\n      <td>3</td>\n      <td>[eight, academy, nomination, beyond, belief, t...</td>\n    </tr>\n    <tr>\n      <th>24999</th>\n      <td>Not that I dislike childrens movies, but this ...</td>\n      <td>3</td>\n      <td>[dislike, childrens, movie, tearjerker, redeem...</td>\n    </tr>\n  </tbody>\n</table>\n<p>25000 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "se = pd.Series(clean_words)\n",
    "train_df['clean_words'] = se.values\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              review  rating  \\\n",
       "0  great little thriller. expecting type silly ho...     8.0   \n",
       "1  nothing could saved movie, even superman.ten y...     1.0   \n",
       "2  good movie. typical war flick something bit di...     8.0   \n",
       "3  pen richard condon (the manchurian candidate 1...     2.0   \n",
       "4  suppose today film relevance early sofia loren...     4.0   \n",
       "\n",
       "                                           rev_tfidf  \n",
       "0  [0.13439975755456773, -0.0010709264420262978, ...  \n",
       "1  [0.0761370059609401, 0.015579074800994696, 0.1...  \n",
       "2  [0.02997489124833944, -0.01540417098875059, -0...  \n",
       "3  [-0.08039615132699425, -0.0756867182493885, 0....  \n",
       "4  [-0.034502646601059506, 0.04037390429403681, -...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>rating</th>\n      <th>rev_tfidf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>great little thriller. expecting type silly ho...</td>\n      <td>8.0</td>\n      <td>[0.13439975755456773, -0.0010709264420262978, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>nothing could saved movie, even superman.ten y...</td>\n      <td>1.0</td>\n      <td>[0.0761370059609401, 0.015579074800994696, 0.1...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>good movie. typical war flick something bit di...</td>\n      <td>8.0</td>\n      <td>[0.02997489124833944, -0.01540417098875059, -0...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>pen richard condon (the manchurian candidate 1...</td>\n      <td>2.0</td>\n      <td>[-0.08039615132699425, -0.0756867182493885, 0....</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>suppose today film relevance early sofia loren...</td>\n      <td>4.0</td>\n      <td>[-0.034502646601059506, 0.04037390429403681, -...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "sw = stopwords.words(\"english\")\n",
    "pca = PCA(n_components=1000)\n",
    "\n",
    "df = train_df.sample(n=1000, random_state=42)\n",
    "df = df.reset_index(drop=True)\n",
    "df.rating = df.rating.astype(\"float\")\n",
    "\n",
    "df.review = df.review.apply(lambda t: \" \".join([t for t in t.replace(\"<br />\", \"\")\n",
    "                                         .lower()\n",
    "                                         .split(\" \") if not t in sw])\n",
    "                                         )\n",
    "\n",
    "tf = text.TfidfVectorizer()\n",
    "X = tf.fit_transform(df['review'])\n",
    "X = X.toarray()\n",
    "\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "df[\"rev_tfidf\"] = [x for x in X_pca]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=X.shape[-1]))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/25\n",
      "1000/1000 [==============================] - 1s 818us/step - loss: 2.6988 - accuracy: 0.1550\n",
      "Epoch 2/25\n",
      "1000/1000 [==============================] - 1s 782us/step - loss: 1.9721 - accuracy: 0.1510\n",
      "Epoch 3/25\n",
      "1000/1000 [==============================] - 1s 760us/step - loss: 2.1027 - accuracy: 0.1460\n",
      "Epoch 4/25\n",
      "1000/1000 [==============================] - 1s 673us/step - loss: 2.1136 - accuracy: 0.1580\n",
      "Epoch 5/25\n",
      "1000/1000 [==============================] - 1s 697us/step - loss: 2.2761 - accuracy: 0.1400\n",
      "Epoch 6/25\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 2.2371 - accuracy: 0.1480\n",
      "Epoch 7/25\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 2.0751 - accuracy: 0.1490\n",
      "Epoch 8/25\n",
      "1000/1000 [==============================] - 1s 669us/step - loss: 2.3117 - accuracy: 0.1510\n",
      "Epoch 9/25\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 2.1818 - accuracy: 0.1520\n",
      "Epoch 10/25\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 2.1338 - accuracy: 0.1460\n",
      "Epoch 11/25\n",
      "1000/1000 [==============================] - 1s 726us/step - loss: 2.1064 - accuracy: 0.1510\n",
      "Epoch 12/25\n",
      "1000/1000 [==============================] - 1s 756us/step - loss: 2.0059 - accuracy: 0.1500\n",
      "Epoch 13/25\n",
      "1000/1000 [==============================] - 1s 682us/step - loss: 2.0768 - accuracy: 0.1500\n",
      "Epoch 14/25\n",
      "1000/1000 [==============================] - 1s 704us/step - loss: 1.9675 - accuracy: 0.1540\n",
      "Epoch 15/25\n",
      "1000/1000 [==============================] - 1s 706us/step - loss: 2.1435 - accuracy: 0.1470\n",
      "Epoch 16/25\n",
      "1000/1000 [==============================] - 1s 790us/step - loss: 2.0378 - accuracy: 0.1540\n",
      "Epoch 17/25\n",
      "1000/1000 [==============================] - 1s 788us/step - loss: 2.1225 - accuracy: 0.1530\n",
      "Epoch 18/25\n",
      "1000/1000 [==============================] - 1s 836us/step - loss: 2.0180 - accuracy: 0.1470\n",
      "Epoch 19/25\n",
      "1000/1000 [==============================] - 1s 710us/step - loss: 1.9875 - accuracy: 0.1500\n",
      "Epoch 20/25\n",
      "1000/1000 [==============================] - 1s 821us/step - loss: 1.9283 - accuracy: 0.1440\n",
      "Epoch 21/25\n",
      "1000/1000 [==============================] - 1s 816us/step - loss: 2.0332 - accuracy: 0.1500\n",
      "Epoch 22/25\n",
      "1000/1000 [==============================] - 1s 799us/step - loss: 1.8884 - accuracy: 0.1480\n",
      "Epoch 23/25\n",
      "1000/1000 [==============================] - 1s 792us/step - loss: 2.0009 - accuracy: 0.1470\n",
      "Epoch 24/25\n",
      "1000/1000 [==============================] - 1s 785us/step - loss: 2.1335 - accuracy: 0.1500\n",
      "Epoch 25/25\n",
      "1000/1000 [==============================] - 1s 771us/step - loss: 1.8406 - accuracy: 0.1490\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad7b70a100>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(42)\n",
    "model.fit(x=X, y=df.rating, batch_size=1, epochs=25) #, callbacks=[loss_stopper]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.076"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df_test = test_df.sample(n=1000, random_state=42)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_test.rating = df_test.rating.astype(\"float\")\n",
    "\n",
    "tf = text.TfidfVectorizer()\n",
    "Xt = tf.fit_transform(df_test['review'])\n",
    "Xt = Xt.toarray()\n",
    "\n",
    "Xt = pca.fit_transform(Xt)\n",
    "\n",
    "preds = model.predict(Xt)\n",
    "\n",
    "preds = preds.flatten()\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    preds[i] = round(preds[i])\n",
    "\n",
    "accuracy_score(preds, df_test.rating.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              review  rating  \\\n",
       "0  Great little thriller. I was expecting some ty...     8.0   \n",
       "1  Nothing could have saved this movie, not even ...     1.0   \n",
       "2  This was a good movie. It wasn't your typical ...     8.0   \n",
       "3  From the pen of Richard Condon (The Manchurian...     2.0   \n",
       "4  I suppose that today this film has relevance b...     4.0   \n",
       "\n",
       "                                           rev_token  \n",
       "0  [Great, little, thriller, ., I, was, expecting...  \n",
       "1  [Nothing, could, have, saved, this, movie, ,, ...  \n",
       "2  [This, was, a, good, movie, ., It, was, n't, y...  \n",
       "3  [From, the, pen, of, Richard, Condon, (, The, ...  \n",
       "4  [I, suppose, that, today, this, film, has, rel...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>rating</th>\n      <th>rev_token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Great little thriller. I was expecting some ty...</td>\n      <td>8.0</td>\n      <td>[Great, little, thriller, ., I, was, expecting...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Nothing could have saved this movie, not even ...</td>\n      <td>1.0</td>\n      <td>[Nothing, could, have, saved, this, movie, ,, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>This was a good movie. It wasn't your typical ...</td>\n      <td>8.0</td>\n      <td>[This, was, a, good, movie, ., It, was, n't, y...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>From the pen of Richard Condon (The Manchurian...</td>\n      <td>2.0</td>\n      <td>[From, the, pen, of, Richard, Condon, (, The, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I suppose that today this film has relevance b...</td>\n      <td>4.0</td>\n      <td>[I, suppose, that, today, this, film, has, rel...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "def get_tag(token):\n",
    "    \n",
    "    tags = []\n",
    "    \n",
    "    for tag in nltk.pos_tag(token):\n",
    "        tags.append(tag[1])\n",
    "    \n",
    "    return tags\n",
    "\n",
    "df = train_df.sample(n=1000, random_state=42)\n",
    "df = df.reset_index(drop=True)\n",
    "df.rating = df.rating.astype(\"float\")\n",
    "\n",
    "df[\"rev_token\"] = df[\"review\"].apply(lambda x: nltk.word_tokenize(x))\n",
    "# df[\"rev_tag\"] = df[\"rev_token\"].apply(lambda x: get_tag(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lexicon(token_seqs, min_freq=1):\n",
    "    token_counts = {}\n",
    "    for seq in token_seqs:\n",
    "        for token in seq:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "    lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "\n",
    "    lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "    lexicon[u'<UNK>'] = 1 \n",
    "    lexicon_size = len(lexicon)\n",
    "\n",
    "    return lexicon\n",
    "\n",
    "rev_lexicon = make_lexicon(df['rev_token'])\n",
    "# tag_lexicon = make_lexicon(df['rev_tag'])\n",
    "\n",
    "def get_lexicon_lookup(lexicon):\n",
    "\n",
    "    lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "    return lexicon_lookup\n",
    "\n",
    "def tokens_to_idxs(token_seqs, lexicon):\n",
    "    idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for token in token_seq] for token_seq in token_seqs]\n",
    "    return idx_seqs\n",
    "\n",
    "df['Sentence_Idxs'] = tokens_to_idxs(df['rev_token'], rev_lexicon)\n",
    "# df['Tag_Idxs'] = tokens_to_idxs(df['rev_tag'], tag_lexicon)\n",
    "\n",
    "# tags_lexicon_lookup = get_lexicon_lookup(tag_lexicon)\n",
    "\n",
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs\n",
    "\n",
    "max_seq_len = max([len(idx_seq) for idx_seq in df['Sentence_Idxs']])\n",
    "\n",
    "train_padded_words = pad_idx_seqs(df['Sentence_Idxs'], max_seq_len + 1)\n",
    "# train_padded_tags = pad_idx_seqs(df['Tag_Idxs'], max_seq_len + 1)\n",
    "\n",
    "def create_model(seq_input_len, n_input_nodes, n_embedding_nodes, n_hidden_nodes, stateful=False, batch_size=20):\n",
    "    \n",
    "    input_layer = Input(shape=(None,))\n",
    "    \n",
    "    #Layer 2\n",
    "    embedding_layer = Embedding(input_dim=n_input_nodes,\n",
    "                                output_dim=n_embedding_nodes,\n",
    "                                mask_zero=True)(input_layer) \n",
    "    \n",
    "    # Layer 3\n",
    "    gru_layer = GRU(units=n_hidden_nodes)(embedding_layer)\n",
    "\n",
    "    #Layer 4\n",
    "    output_layer = Dense(units=1)(gru_layer)\n",
    "\n",
    "    model = Model(inputs=[input_layer], outputs=output_layer)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "50/50 [==============================] - 201s 4s/step - loss: 39.6991\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 187s 4s/step - loss: 11.5358\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 188s 4s/step - loss: 6.8722\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 188s 4s/step - loss: 2.0475\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 187s 4s/step - loss: 1.2633\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad6cfc2040>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "model = create_model(seq_input_len=train_padded_words.shape[-1] - 1,\n",
    "                     n_input_nodes=len(rev_lexicon) + 1,\n",
    "                     n_embedding_nodes=300,\n",
    "                     n_hidden_nodes=500)\n",
    "\n",
    "model.fit(x=train_padded_words[:,1:], y=df.rating, batch_size=20, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat([test_pos, test_neg], ignore_index=True)\n",
    "\n",
    "test_df = test_df.sample(n=1000, random_state=42)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "test_df[\"rev_token\"] = test_df[\"review\"].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "\n",
    "test_rev_lexicon = make_lexicon(test_df['rev_token'])\n",
    "\n",
    "test_df['Sentence_Idxs'] = tokens_to_idxs(test_df['rev_token'], test_rev_lexicon)\n",
    "\n",
    "max_seq_len = max([len(idx_seq) for idx_seq in test_df['Sentence_Idxs']])\n",
    "\n",
    "test_padded_words = pad_idx_seqs(test_df['Sentence_Idxs'], max_seq_len + 1)\n",
    "\n",
    "preds = model.predict(test_padded_words[:,1:])\n",
    "\n",
    "preds = preds.flatten()\n",
    "for i in range(len(preds)):\n",
    "    preds[i] = round(preds[i])\n",
    "\n",
    "accuracy_score(preds, test_df.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 10s 1us/step\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 50)                500050    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 505,201\n",
      "Trainable params: 505,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "80/80 [==============================] - 7s 66ms/step - loss: 0.5296 - accuracy: 0.7321 - val_loss: 0.2620 - val_accuracy: 0.8935\n",
      "Epoch 2/2\n",
      "80/80 [==============================] - 3s 35ms/step - loss: 0.2159 - accuracy: 0.9191 - val_loss: 0.2642 - val_accuracy: 0.8951\n",
      "Test-Accuracy: 0.8693374991416931\n"
     ]
    }
   ],
   "source": [
    "# also\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
    "def vectorize(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "data = vectorize(data)\n",
    "targets = np.array(targets).astype(\"float32\")\n",
    "test_x = data[:10000]\n",
    "test_y = targets[:10000]\n",
    "train_x = data[10000:]\n",
    "train_y = targets[10000:]\n",
    "model = models.Sequential()\n",
    "# Input - Layer\n",
    "model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000, )))\n",
    "# Hidden - Layers\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()\n",
    "# compiling the model\n",
    "model.compile(\n",
    " optimizer = \"adam\",\n",
    " loss = \"binary_crossentropy\",\n",
    " metrics = [\"accuracy\"]\n",
    ")\n",
    "results = model.fit(\n",
    " train_x, train_y,\n",
    " epochs= 2,\n",
    " batch_size = 500,\n",
    " validation_data = (test_x, test_y)\n",
    ")\n",
    "print(\"Test-Accuracy:\", np.mean(results.history[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-chosen",
   "metadata": {},
   "source": [
    "# 2. (evil) XOR Problem\n",
    "\n",
    "Train an LSTM to solve the XOR problem: that is, given a sequence of bits, determine its parity. The LSTM should consume the sequence, one bit at a time, and then output the correct answer at the sequence’s end. Test the two approaches below:\n",
    "\n",
    "### 2.1 \n",
    "\n",
    "Generate a dataset of random <=100,000 binary strings of equal length <= 50. Train the LSTM; what is the maximum length you can train up to with precisison?\n",
    "    \n",
    "\n",
    "### 2.2\n",
    "\n",
    "Generate a dataset of random <=200,000 binary strings, where the length of each string is independently and randomly chosen between 1 and 50. Train the LSTM. Does it succeed? What explains the difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://vitez.me/lstm-xor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 50\n",
    "COUNT = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_pair = lambda x: [x, not(x)]\n",
    "training = np.array([[bin_pair(random.choice([0, 1])) for _ in range(SEQ_LEN)] for _ in range(COUNT)])\n",
    "target = np.array([[bin_pair(x) for x in np.cumsum(example[:,0]) % 2] for example in training])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape check: (100000, 50, 2) = (100000, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "print('shape check:', training.shape, '=', target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(SEQ_LEN, 2), dtype='float32'))\n",
    "model.add(LSTM(1, return_sequences=True))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 10s 10ms/step - loss: 0.6934 - accuracy: 0.4983\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.6930 - accuracy: 0.4933\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.6905 - accuracy: 0.5092\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.5110 - accuracy: 0.8164\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.2108 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.1430 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.1048 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.0796 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.0615 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.0482 - accuracy: 1.0000\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 50, 1)             16        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50, 2)             4         \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(training, target, epochs=10, batch_size=128)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "randomly selected sequence: [0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1\n 1 1 1 0 0 1 1 1 0 1 1 0 0]\nprediction: 0\nconfidence: 100.00%\nactual: 0\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(training)\n",
    "i = random.randint(0, COUNT)\n",
    "chance = predictions[i,-1,0]\n",
    "print('randomly selected sequence:', training[i,:,0])\n",
    "print('prediction:', int(chance > 0.5))\n",
    "print('confidence: {:0.2f}%'.format((chance if chance > 0.5 else 1 - chance) * 100))\n",
    "print('actual:', np.sum(training[i,:,0]) % 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 50\n",
    "COUNT = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_pair = lambda x: [x, not(x)]\n",
    "training = np.array([[bin_pair(random.choice([0, 1])) for _ in range(SEQ_LEN)] for _ in range(COUNT)])\n",
    "target = np.array([[bin_pair(x) for x in np.cumsum(example[:,0]) % 2] for example in training])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(SEQ_LEN, 2), dtype='float32'))\n",
    "model.add(LSTM(1, return_sequences=True))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 16s 9ms/step - loss: 0.6931 - accuracy: 0.5097\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.6659 - accuracy: 0.5539\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.1852 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0948 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0567 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0353 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 50, 1)             16        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50, 2)             4         \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(training, target, epochs=10, batch_size=128)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "randomly selected sequence: [1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 0\n 1 1 1 1 1 1 0 0 0 0 0 1 1]\nprediction: 1\nconfidence: 99.99%\nactual: 1\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(training)\n",
    "i = random.randint(0, COUNT)\n",
    "chance = predictions[i,-1,0]\n",
    "print('randomly selected sequence:', training[i,:,0])\n",
    "print('prediction:', int(chance > 0.5))\n",
    "print('confidence: {:0.2f}%'.format((chance if chance > 0.5 else 1 - chance) * 100))\n",
    "print('actual:', np.sum(training[i,:,0]) % 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pd.set_option('display.max_colwidth', 170) #widen pandas rows display\n",
    "\n",
    "# Get Spacy english core model\n",
    "# Need to run \"python -m spacy download en_core_web_sm\" first\n",
    "encoder = spacy.load(\"en_core_web_sm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}